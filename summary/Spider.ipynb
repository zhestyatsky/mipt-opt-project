{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider и SpiderBoost\n",
    "\n",
    "#### стохастические алгоритмы с уменьшенной дисперсией\n",
    "\n",
    "\n",
    "## Введение\n",
    "\n",
    "Здесь рассмотрен вычислительный метод SPIDER, применимый для различных задач, а также использование этого на примере алгоритма SpiderBoost\n",
    "\n",
    "# Spider\n",
    "\n",
    "#### техника решения задачи невыпуклой оптимизации\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "Рассматривается задача оптимизации\n",
    "\n",
    "\n",
    "$$\\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}) \\equiv \\mathbb{E}(F(\\mathbf{x}, \\zeta))$$\n",
    "\n",
    "Здесь $F(\\mathbf{x}, \\zeta)$ – гладкая, в общем случае невыпуклая,\n",
    "\n",
    "$\\zeta$ – случайный вектор, индексирующий функцию\n",
    "\n",
    "В частности, рассматривается задача с конечным числом индексов:\n",
    "\n",
    "$$\\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{\\mathbf{x}}) = \\frac{1}{n}\\Sigma^n_{i = 1}f_i(\\mathbf{\\mathbf{x}})$$\n",
    "\n",
    "Для простоты записи так будем обозначать задачи как в оффлайн случае (с конечным набором функций), так и в онлайн случае (с бесконечным набором, заданным $\\zeta$)\n",
    "\n",
    "\n",
    "## Мотивация\n",
    "\n",
    "SpiderBoost предполагает скорость сходимости $O(\\min(n^{\\frac{1}{2}}\\epsilon^{-2}, \\epsilon^{-3}))$ в смысле суммарного количества подсчитанных градиентов. Для класса функций стоимость подсчета не может быть улучшена \n",
    "\n",
    "SGD добивается только скорости $O(\\min(n\\epsilon^{-2}, \\epsilon^{-4}))$\n",
    "\n",
    "SVRG имеет стоимость $O(\\min(n^{\\frac{2}{3}}\\epsilon^{-2}, \\epsilon^{-3.(3)}))$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Основная идея SPIDER\n",
    "\n",
    "SPIDER – метод первого порядка, основанный на аппарате SGD за исключением особенностей:\n",
    "\n",
    "1. На некоторых итерациях будем просчитывать градиент полностью, как в алгоритме GD\n",
    "\n",
    "2. На всех остальных шагах (таких будет большинство) мы станем отслеживать предварительно введенную функцию $\\nu$, зависящую от градиента мини-батча (вместо самого градиента, как предполагает SGD)\n",
    "\n",
    "Наша задача добиться таким образом, с одной стороны, хорошей точности результата посредством (1), и, с другой стороны, высокой производительности посредством (2).\n",
    "\n",
    "## Оценка погрешности метода \n",
    "\n",
    "Воспользуемся вспомогательной оценкой. \n",
    "\n",
    "Пусть $B(\\mathbf{x})$ отображает $\\mathbf{x} \\in \\mathbb{R}^d$ на случайную  функцию $B_i(\\mathbf{x})$: в алгоритмах под $B(x)$ будет подразумеваться градиент\n",
    "\n",
    "1. Пусть функция $\\nu$ такова, что\n",
    "\n",
    "$$\\mathbb{E}[B(\\mathbf{x}^k) - B(\\mathbf{x}^{k - 1}) \\mid \\mathbf{x}_{0:k}] = \\nu^k - \\nu^{k - 1}$$\n",
    "\n",
    "где $\\mathbf{x}_{0:k} = \\mathbf{x}_1 \\dots \\mathbf{x}_k$\n",
    "\n",
    "2. Пусть также выполнено\n",
    "\n",
    "$$\\mathbb{E} \\Arrowvert B_i(x) - B_i(y) \\Arrowvert^2 \\leq L_B^2\\Arrowvert x - y \\Arrowvert^2$$\n",
    "\n",
    "для $\\Arrowvert \\mathbf{x}^k - \\mathbf{x}^{k-1} \\Arrowvert \\leq \\epsilon_1$, $ \\forall k \\in 1 \\dots K$\n",
    "\n",
    "3. Наконец, пусть \n",
    "\n",
    "$$\\nu: \\nu^k = {B_{S_*}(\\mathbf{x}^k) - {B_{S_*}(\\mathbf{x^{k-1}}) + \\nu^{k - 1}$$\n",
    "\n",
    "здесь $S_*$ –  мощность батча\n",
    "\n",
    "тогда имеет место:\n",
    "\n",
    "#### Оценка для ошибки $\\nu^k$ в терминах дисперсии\n",
    "\n",
    "$$\\mathbb{E}\\Arrowvert \\nu^k - B(\\mathbf{x}^k) \\Arrowvert^2 \\leq \\frac{k L^2_B \\epsilon^2_1}{S_*} + \\mathbb{E}\\Arrowvert \\nu^0 - B(\\mathbf{x}^0) \\Arrowvert^2$$\n",
    "\n",
    "ВАЖНО: т.е. при удачном выборе функции $\\nu$, а также шага в зависимости от $L_B$ значение построенной функции $\\nu^k$ с хорошей точностью приближается к значению, получаемом при GD: $B(\\mathbf{x}^k)$; и есть предпосылка сэкономить на скорости вычисления $\\nu$ вместо вычисления полного градиента.\n",
    "\n",
    "Скорости сходимости метода зависят от выбора функции $\\nu$, в следующей секции рассмотрим алгоритм, в котором $\\nu$ хорошо подобрана:\n",
    "\n",
    "\n",
    "# SpiderBoost \n",
    "\n",
    "#### Алгоритм решения задачи невыпуклой оптимизации посредством Spider\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "Рассматривается задача оптимизации на конечном наборе функций:\n",
    "\n",
    "$$\\min_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{\\mathbf{x}}) = \\frac{1}{n}\\Sigma^n_{i = 1}f_i(\\mathbf{\\mathbf{x}})$$\n",
    "\n",
    "где функции $f_i(\\mathbf{x})$ в общем случае невыпуклые\n",
    "\n",
    "\n",
    "\n",
    "## Мотивация \n",
    "\n",
    "\n",
    "\n",
    "#### допустимый шаг для SpiderBoost\n",
    "\n",
    "$\\eta = O(\\frac{1}{L})$, это значение превосходит оценки для других известных алгоритмов основанных на Spider: $\\eta = O(\\frac{\\epsilon}{L})$\n",
    "\n",
    "#### оценка сложности алгоритма SpiderBoost: \n",
    "\n",
    "$O(\\sqrt{n}\\epsilon^{-2} + n)$ -- минимальное возможное значение при соблюдении $n \\leq O(\\epsilon^{-4})$\n",
    "\n",
    "\n",
    "## Алгоритм SpiderBoost\n",
    "\n",
    "### Основная идея\n",
    "\n",
    "Воспользуемся методом предложенным в Spider выбрав функцию $\\nu:$\n",
    "\n",
    "1. $\\nu_0 = \\nabla f(x_0)$\n",
    "\n",
    "2. $\\nu_k = \\frac{1}{\\mid S\\mid} \\Sigma_{i \\in S} [\\nabla f_i(x_k) - \\nabla f_i(x_{k - 1}) + \\nu_{k - 1}]$\n",
    "\n",
    "более подробно: \n",
    "\n",
    "### Описание алгоритма \n",
    "\n",
    "– $\\mathbf{input}: \\eta = \\frac{1}{2L}; q, K, \\mid S \\mid \\in \\mathbb{N}$\n",
    "\n",
    "– $\\mathbf{for}$ $k \\in 1 \\dots K$:\n",
    "\n",
    "– – $\\mathbf{if}$ $\\mod(k, q) = 0 \\Rightarrow \\nu_k = \\nabla f(x_k)$\n",
    "\n",
    "– – $\\mathbf{else} \\Rightarrow$ $\\mathbf{draw} \\mid S \\mid$ samples with replacement, $\\mathbf{compute}$ $\\nu_k$\n",
    "\n",
    "– $\\mathbf{calculate}$ $x_{k+1} = x_k - \\eta \\nu_k$\n",
    "\n",
    "– $\\mathbf{return}$ $x_{\\xi}, \\xi \\stackrel{Unif}{\\sim} \\{ 0 \\dots K - 1 \\}$\n",
    "\n",
    "\n",
    "### Ограничения на целевую функцию\n",
    "\n",
    "1. Функция ограничена снизу $\\exists\\inf f(\\mathbf{x}) > -\\infty$ \n",
    "\n",
    "2. Каждый градиент $\\nabla f_i$ – $L$-Липшицовый: $\\forall x, y \\in \\mathbb{R}^d, \\Arrowvert \\nabla f_i(x) - \\nabla f_i(y) \\Arrowvert \\leq L \\Arrowvert x - y\\Arrowvert \\forall I \\in 1 \\dots n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
