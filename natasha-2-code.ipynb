{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = \\frac{1}{n}\\sum_{i=1}^n f_i(x) = \\frac{1}{n}\\Vert Ax-b \\Vert^2$$ \n",
    "$$f_i(x) = (A[i]x - b[i])^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = [i+1 for i in range(1000)]\n",
    "n = 1000\n",
    "m = 20\n",
    "\n",
    "A = torch.randn(n,m)\n",
    "x_sol = torch.randn(m)\n",
    "b = A@x_sol + torch.empty(n).normal_(mean=0,std=1)\n",
    "\n",
    "def f_i(x, i):\n",
    "    return (A[i,:]@x - b[i])**2\n",
    "\n",
    "F = lambda x : [f_i(x, i) for i in range(n)]\n",
    "f = lambda x : torch.stack(F(x), dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(F(x_sol), dim=0).mean(dim=0), torch.norm(torch.matmul(A, x_sol) - b)**2/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = torch.symeig(torch.matmul(2/n*A.T, A), eigenvectors = False)\n",
    "L = torch.max(eig[0]) #max eigenvalue\n",
    "\n",
    "\n",
    "L_2 = 1\n",
    "\n",
    "nu = 0\n",
    "def var(x_0):\n",
    "    x = x_0.clone().requires_grad_()#torch.rand_like(x_0, requires_grad = True)\n",
    "    f_x = f(x)\n",
    "    f(x).backward()    \n",
    "    I = torch.randint(n, (50,))\n",
    "    Nu = []\n",
    "    for k in range(50):\n",
    "        x_i = x_0.clone().requires_grad_()\n",
    "        f_ix = f_i(x_i, I[k])\n",
    "        f_ix.backward()\n",
    "        Nu.append(torch.norm(x.grad - x_i.grad)**2)\n",
    "    return torch.stack(Nu, dim=0).mean(dim=0)      \n",
    "for k in range(50):\n",
    "    nu = max(nu, var(torch.randn(m)))\n",
    "    \n",
    "L, L_2, nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "eps = 1e-1\n",
    "delta = eps**0.25\n",
    "\n",
    "B = int(eps**-2) # batch size\n",
    "T = int(delta**-2) #oja iterations\n",
    "N_1 = int(1/delta/eps)\n",
    "\n",
    "ds = TensorDataset(A, b)\n",
    "\n",
    "dl_1 = DataLoader(ds, 1, shuffle=True)\n",
    "dl_T = DataLoader(ds, T, shuffle=True)\n",
    "dl_B = DataLoader(ds, B, shuffle=True)\n",
    "\n",
    "loss_fn =  torch.nn.functional.mse_loss\n",
    "\n",
    "\n",
    "y_0 = torch.ones(m)\n",
    "\n",
    "X = []\n",
    "k = 0\n",
    "y_k = y_0.clone().requires_grad_()\n",
    "Y = [y_0]\n",
    "while(True):    \n",
    "    \n",
    "    #Oja's algorithm\n",
    "    \n",
    "    eta = np.sqrt(T)\n",
    "    \n",
    "    Eig = []\n",
    "    \n",
    "    for j in range(10): \n",
    "    \n",
    "        W = []\n",
    "        w_1 = torch.empty(m).normal_(mean=0,std=1)\n",
    "        W.append(w_1/torch.norm(w_1))\n",
    "        for i in range(1, T):\n",
    "            w_last = W[-1] \n",
    "\n",
    "            A_i, b_i = next(iter(dl_1))\n",
    "\n",
    "            y = y_k.clone().requires_grad_()\n",
    "            loss = loss_fn(A_i@y, b_i)\n",
    "            grad, = torch.autograd.grad(loss, y, create_graph=True)\n",
    "            z = grad @ w_last\n",
    "            prod, = torch.autograd.grad(z, y)\n",
    "            w = w_last - eta/L*prod\n",
    "\n",
    "            W.append(w/torch.norm(w))\n",
    "        v = W[torch.randint(T, (1,))]\n",
    "    \n",
    "        A_i, b_i = next(iter(dl_T))\n",
    "        y = y_k.clone().requires_grad_()\n",
    "        loss = loss_fn(A_i@y, b_i)\n",
    "        grad, = torch.autograd.grad(loss, y, create_graph=True)\n",
    "        z = grad @ v\n",
    "        prod, = torch.autograd.grad(z, y)\n",
    "    \n",
    "        Eig.append([v, v@prod])\n",
    "    Eig.sort(key = lambda x : x[1])\n",
    "    v, prod = Eig[0]\n",
    "    \n",
    "    \n",
    "    if prod <= -delta/2:\n",
    "        with torch.no_grad():\n",
    "            if int(torch.randint(2, (1,))) == 1:\n",
    "                y_k += delta/L_2*v\n",
    "            else:\n",
    "                y_k -= delta/L_2*v\n",
    "    else:\n",
    "        k+=1\n",
    "        \n",
    "        #Natasha 1.5\n",
    "        \n",
    "        alpha = eps*delta\n",
    "        reg_k = lambda x : L*(max(0, torch.norm(x-y_k) - delta/L_2 ))**2 #F^k(x) = f(x) + reg_k(x)\n",
    "      \n",
    "        p = int(10*(delta/eps/L)**(2*0.3333)) #sub-epochs amount      \n",
    "        _m = int(B/p)\n",
    "        \n",
    "        X=[]\n",
    "        \n",
    "        A_i, b_i = next(iter(dl_B))\n",
    "        loss = loss_fn(A_i@y_k, b_i)\n",
    "        mu, = torch.autograd.grad(loss, y_k)\n",
    "#         with torch.no_grad():\n",
    "#             y_k.grad.zero_()\n",
    "        x_hat = y_k.clone()\n",
    "        for s in range(p):\n",
    "            x = [x_hat.clone()]\n",
    "            X.append(x_hat)\n",
    "            for t in range(_m):\n",
    "                A_i, b_i = next(iter(dl_1))\n",
    "                x_t = x[t].clone().requires_grad_()\n",
    "                F1 = loss_fn(A_i@x_t, b_i) + reg_k(x_t)\n",
    "                grad1, = torch.autograd.grad(F1, x_t)\n",
    "                \n",
    "                F2 = loss_fn(A_i@y_k, b_i) + reg_k(y_k)\n",
    "                grad2, = torch.autograd.grad(F2, y_k)\n",
    "                nabla = grad1 - grad2 + mu + 2*delta*(x_t-x_hat)\n",
    "                \n",
    "                x.append(x_t - alpha*nabla)\n",
    "    \n",
    "            x_hat = torch.stack(x, dim=0).mean(dim=0)\n",
    "                   \n",
    "        y_k = X[torch.randint(len(X), (1,))]\n",
    "        Y.append(y_k.clone().detach())\n",
    "                \n",
    "    if k == N_1:\n",
    "        break\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [loss_fn(A@y, b) for y in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
