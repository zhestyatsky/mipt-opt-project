# mipt-opt-project

![GitHub Logo](/images/Gradient_ascent.png)
Format: ![Alt Text](url)

## Использованные статьи

* "Natasha 2: Faster non-convex optimization than sgd." <br>
https://papers.nips.cc/paper/7533-natasha-2-faster-non-convex-optimization-than-sgd.pdf
* "SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms." <br>
https://papers.nips.cc/paper/8511-spiderboost-and-momentum-faster-variance-reduction-algorithms.pdf
* "SPIDER : Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator" <br>
https://papers.nips.cc/paper/7349-spider-near-optimal-non-convex-optimization-via-stochastic-path-integrated-differential-estimator.pdf
